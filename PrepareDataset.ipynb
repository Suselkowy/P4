{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1706510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nids_datasets import Dataset\n",
    "import pandas as pd\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf21f10",
   "metadata": {},
   "source": [
    "### 1. Pobranie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82208c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BENIGN  FTP-Patator  SSH-Patator  DoS slowloris  DoS Slowhttptest  \\\n",
      "File                                                                       \n",
      "1     4822905            0            0              0                 0   \n",
      "2     3571097            0            0              0                 0   \n",
      "3     3556526         1953            0              0                 0   \n",
      "4     2575904        12803            0              0                 0   \n",
      "5     3007086        68205            0              0                 0   \n",
      "6     2637797        28602        38680              0                 0   \n",
      "7     2612615            0       124631              0                 0   \n",
      "8     2649076            0            0              0                 0   \n",
      "9     2948428            0            0          47519             19866   \n",
      "10     422225            0            0              0             19675   \n",
      "11    4418585            0            0             41                 0   \n",
      "12    2651388            0            0              0                 0   \n",
      "13    3109436            0            0              0                 0   \n",
      "14    2845250            0            0              0                 0   \n",
      "15    2558990            0            0              0                 0   \n",
      "16    3182740            0            0              0                 0   \n",
      "17    1920222            0            0              0                 0   \n",
      "18     528653            0            0              0                 0   \n",
      "\n",
      "      DoS Hulk  Heartbleed  Web Attack – Brute Force  Web Attack – XSS  \\\n",
      "File                                                                     \n",
      "1            0           0                         0                 0   \n",
      "2            0           0                         0                 0   \n",
      "3            0           0                         0                 0   \n",
      "4            0           0                         0                 0   \n",
      "5            0           0                         0                 0   \n",
      "6            0           0                         0                 0   \n",
      "7            0           0                         0                 0   \n",
      "8            0           0                         0                 0   \n",
      "9            0           0                         0                 0   \n",
      "10     1459567           0                         0                 0   \n",
      "11      786683       49296                         0                 0   \n",
      "12           0           0                     12370                 0   \n",
      "13           0           0                     17873              9344   \n",
      "14           0           0                         0                 0   \n",
      "15           0           0                         0                 0   \n",
      "16           0           0                         0                 0   \n",
      "17           0           0                         0                 0   \n",
      "18           0           0                         0                 0   \n",
      "\n",
      "      Web Attack – SQL Injection  ...  dos  fuzzers  generic  reconnaissance  \\\n",
      "File                              ...                                          \n",
      "1                              0  ...    0        0        0               0   \n",
      "2                              0  ...    0        0        0               0   \n",
      "3                              0  ...    0        0        0               0   \n",
      "4                              0  ...    0        0        0               0   \n",
      "5                              0  ...    0        0        0               0   \n",
      "6                              0  ...    0        0        0               0   \n",
      "7                              0  ...    0        0        0               0   \n",
      "8                              0  ...    0        0        0               0   \n",
      "9                              0  ...    0        0        0               0   \n",
      "10                             0  ...    0        0        0               0   \n",
      "11                             0  ...    0        0        0               0   \n",
      "12                             0  ...    0        0        0               0   \n",
      "13                           126  ...    0        0        0               0   \n",
      "14                             0  ...    0        0        0               0   \n",
      "15                             0  ...    0        0        0               0   \n",
      "16                             0  ...    0        0        0               0   \n",
      "17                             0  ...    0        0        0               0   \n",
      "18                             0  ...    0        0        0               0   \n",
      "\n",
      "      worms  shellcode  backdoor  analysis  DoS GoldenEye    total  \n",
      "File                                                                \n",
      "1         0          0         0         0              0  4822905  \n",
      "2         0          0         0         0              0  3571097  \n",
      "3         0          0         0         0              0  3558479  \n",
      "4         0          0         0         0              0  2588707  \n",
      "5         0          0         0         0              0  3075291  \n",
      "6         0          0         0         0              0  2705079  \n",
      "7         0          0         0         0              0  2737246  \n",
      "8         0          0         0         0              0  2649076  \n",
      "9         0          0         0         0              0  3015813  \n",
      "10        0          0         0         0              0  1901467  \n",
      "11        0          0         0         0         104513  5359118  \n",
      "12        0          0         0         0              0  2663758  \n",
      "13        0          0         0         0              0  3136779  \n",
      "14        0          0         0         0              0  2905004  \n",
      "15        0          0         0         0              0  2558990  \n",
      "16        0          0         0         0              0  3195638  \n",
      "17        0          0         0         0              0  3184487  \n",
      "18        0          0         0         0              0   817864  \n",
      "\n",
      "[18 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(dataset='CIC-IDS2017', subset=['Packet-Fields'], files='all')\n",
    "df_info = DatasetInfo(dataset='CIC-IDS2017')\n",
    "print(df_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d3b61",
   "metadata": {},
   "source": [
    "#### UWAGA - Cały dataset jest bardzo duży, więc pobieranie może zająć dużo czasu i miejsca na dysku (ok. 250GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UWAGA - Cały dataset jest bardzo duży, więc pobieranie może zająć dużo czasu i miejsca na dysku (ok. 250GB).\n",
    "# data.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386074b4",
   "metadata": {},
   "source": [
    "### 2. Przetowrzenie danych z foramtu Parquet do formatu CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4177a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 261 column names to 'cicids2017_columns.txt'\n",
      "First few columns: ['packet_id', 'flow_id', 'source_ip', 'source_port', 'destination_ip']\n"
     ]
    }
   ],
   "source": [
    "# Load the first file to get column names\n",
    "pf = ParquetFile('CIC-IDS2017/Packet-Fields/Packet_Fields_File_1.parquet')\n",
    "\n",
    "if pf.num_row_groups > 0:\n",
    "    # Read just one row to get column structure\n",
    "    first_batch = next(pf.iter_batches(batch_size=1))\n",
    "    df = pa.Table.from_batches([first_batch]).to_pandas()\n",
    "    \n",
    "    # Get column names\n",
    "    columns = list(df.columns)\n",
    "    \n",
    "    # Save to text file\n",
    "    with open('cicids2017_columns.txt', 'w') as f:\n",
    "        for col in columns:\n",
    "            f.write(col + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(columns)} column names to 'cicids2017_columns.txt'\")\n",
    "    print(f\"First few columns: {columns[:5]}\")\n",
    "\n",
    "else:\n",
    "    print(\"File has no data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5eac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_1.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_1.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_1.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_2.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_2.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_2.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_3.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_3.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_3.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_4.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_4.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_4.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_5.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_5.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_5.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_6.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_6.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_6.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_7.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_7.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_7.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_8.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_8.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_8.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_9.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_9.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_9.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_10.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_10.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_10.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_11.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_11.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_11.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_12.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_12.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_12.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_13.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_13.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_13.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_14.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_14.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_14.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_15.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_15.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_15.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_16.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_16.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_16.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_17.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_17.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_17.csv\n",
      "Przetwarzanie pliku: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_18.parquet\n",
      "  Rozpoczęto konwersję, batch size: 10000\n",
      "  Zakończono konwersję: giga_miejsca/CIC-IDS2017/Packet-Fields/Packet_Fields_File_18.parquet -> giga_miejsca/converted_csv_files/Packet_Fields_File_18.csv\n",
      "Wszystkie pliki zostały przetworzone!\n",
      "\n",
      "Tworzenie archiwum ZIP: packet_fields_csv.zip\n",
      "  Dodano do archiwum: Packet_Fields_File_5.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_15.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_18.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_1.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_12.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_16.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_8.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_14.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_6.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_4.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_3.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_9.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_7.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_2.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_13.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_17.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_11.csv\n",
      "  Dodano do archiwum: Packet_Fields_File_10.csv\n",
      "\n",
      "Archiwum ZIP zostało utworzone: packet_fields_csv.zip\n",
      "Pliki CSV znajdują się w katalogu: giga_miejsca/converted_csv_files\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Kolumny które potrzebujesz\n",
    "needed_columns = [\n",
    "    \"Ethernet type\",\n",
    "    \"IP proto\", \n",
    "    \"IP flags\",\n",
    "    \"TCP dport\",\n",
    "    \"UDP dport\", \n",
    "    \"IP len\",\n",
    "    \"TCP flags\",\n",
    "    \"attack_label\",\n",
    "    \"flow_id\"\n",
    "]   \n",
    "\n",
    "# Ścieżka do plików Parquet\n",
    "parquet_directory = \"giga_miejsca/CIC-IDS2017/Packet-Fields\"\n",
    "\n",
    "# Utwórz katalog dla plików CSV\n",
    "csv_directory = \"giga_miejsca/converted_csv_files\"\n",
    "Path(csv_directory).mkdir(exist_ok=True)\n",
    "\n",
    "# Przetwarzaj wszystkie 18 plików\n",
    "for file_num in range(1, 19):  # Pliki od 1 do 18\n",
    "    input_file = os.path.join(parquet_directory, f'Packet_Fields_File_{file_num}.parquet')\n",
    "    output_file = os.path.join(csv_directory, f'Packet_Fields_File_{file_num}.csv')\n",
    "    \n",
    "    # Sprawdź czy plik istnieje\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Plik {input_file} nie istnieje, pomijam...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Przetwarzanie pliku: {input_file}\")\n",
    "    \n",
    "    # Otwórz plik Parquet\n",
    "    parquet_file = pq.ParquetFile(input_file)\n",
    "    \n",
    "    # Przetwarzaj w batch'ach\n",
    "    batch_size = 10000  # Dostosuj rozmiar batch'a do swojej pamięci\n",
    "    \n",
    "    for i, batch in enumerate(parquet_file.iter_batches(\n",
    "        batch_size=batch_size, \n",
    "        columns=needed_columns\n",
    "    )):\n",
    "        # Konwertuj batch do pandas DataFrame\n",
    "        df_chunk = batch.to_pandas()\n",
    "        \n",
    "        # Ustaw tryb zapisu\n",
    "        mode = 'w' if i == 0 else 'a'\n",
    "        header = i == 0  # Nagłówek tylko dla pierwszego batch'a\n",
    "        \n",
    "        # Zapisz do CSV\n",
    "        df_chunk.to_csv(\n",
    "            output_file,\n",
    "            mode=mode,\n",
    "            header=header,\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        if i == 0:  # Informacja tylko dla pierwszego batcha każdego pliku\n",
    "            print(f\"  Rozpoczęto konwersję, batch size: {batch_size}\")\n",
    "    \n",
    "    print(f\"  Zakończono konwersję: {input_file} -> {output_file}\")\n",
    "\n",
    "print(\"Wszystkie pliki zostały przetworzone!\")\n",
    "\n",
    "# # Utwórz archiwum ZIP z plikami CSV\n",
    "# zip_filename = \"packet_fields_csv.zip\"\n",
    "# print(f\"\\nTworzenie archiwum ZIP: {zip_filename}\")\n",
    "\n",
    "# with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "#     # Dodaj wszystkie pliki CSV do archiwum\n",
    "#     for csv_file in os.listdir(csv_directory):\n",
    "#         if csv_file.endswith('.csv'):\n",
    "#             file_path = os.path.join(csv_directory, csv_file)\n",
    "#             # Dodaj plik do ZIP (bez ścieżki katalogu)\n",
    "#             zipf.write(file_path, csv_file)\n",
    "#             print(f\"  Dodano do archiwum: {csv_file}\")\n",
    "\n",
    "# print(f\"\\nArchiwum ZIP zostało utworzone: {zip_filename}\")\n",
    "# print(f\"Pliki CSV znajdują się w katalogu: {csv_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = 'packet_fields_csv'\n",
    "\n",
    "# List to hold filtered dataframes\n",
    "filtered_dfs = []\n",
    "\n",
    "# Loop through files Packet_Fields_File_X.csv where X is 1 to 18\n",
    "for i in range(1, 19):\n",
    "    file_name = f'Packet_Fields_File_{i}.csv'\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            # Read individual file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Reading {file_name} with {len(df)} rows\")\n",
    "            \n",
    "            # Filter to one random packet per flow_id for this file\n",
    "            filtered_df = df.groupby('flow_id').apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "            \n",
    "            # Add to list\n",
    "            filtered_dfs.append(filtered_df)\n",
    "            \n",
    "            print(f\"  -> Filtered to {len(filtered_df)} unique flow_ids\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_name}\")\n",
    "\n",
    "# Combine all filtered dataframes\n",
    "if filtered_dfs:\n",
    "    final_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "    \n",
    "    # Save the final dataset\n",
    "    output_file = 'filtered_packet_fields.csv'\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nFinal dataset saved as '{output_file}'\")\n",
    "    print(f\"Total rows in final dataset: {len(final_df)}\")\n",
    "    print(f\"Files processed: {len(filtered_dfs)}\")\n",
    "    print(f\"Columns: {list(final_df.columns)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No files were successfully processed.\")\n",
    "\n",
    "zip_filename = \"filtered_packet_fields.zip\"\n",
    "print(f\"\\nTworzenie archiwum ZIP: {zip_filename}\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    file_path = os.path.join(csv_directory, output_file)\n",
    "    zipf.write(file_path, output_file)\n",
    "    print(f\"  Dodano do archiwum: {output_file}\")\n",
    "           \n",
    "print(f\"\\nArchiwum ZIP zostało utworzone: {zip_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
